####################
#
# AI-Log-Analyzer
# Description: Analyzes system logs to detect anomalies using Isolation Forest.
# Author: Alexandre St-fort
# Last modified: 01/02/26
#
####################

import os
import json
import shutil
import requests
import pandas as pd
import numpy as np
from sklearn.ensemble import IsolationForest
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt

LOG_FILE_PATH = "app.log"
SLACK_WEBHOOK_URL = os.getenv("SLACK_URL")
CONTAMINATION_RATE = "10%"
LEVEL_MAPPING = {
    10: 0,  # TRACE
    20: 0,  # DEBUG
    30: 1,  # INFO
    40: 2,  # WARN
    50: 3,  # ERROR
    60: 4   # FATAL
}

def percent_to_decimal(CONTAMINATION_RATE):
    CONTAMINATION_RATE = float(CONTAMINATION_RATE.strip('%'))
    print(CONTAMINATION_RATE)
    return CONTAMINATION_RATE / 100

def load_and_parse_logs(LOG_FILE_PATH):
    data = []

    if not os.path.exists(LOG_FILE_PATH):
        print(f"‚ùå ERROR: File {LOG_FILE_PATH} not found.")
        return pd.DataFrame()

    with open(LOG_FILE_PATH, "r") as file:
        for line in file:
            line = line.strip()
            if not line:
                continue
            
            try:
                # Parse each line as a JSON object
                log_entry = json.loads(line)
                
                # Extract fields: 'time', 'level', and 'msg' (or 'message')
                timestamp = log_entry.get("time")
                # Convert numeric level to string (e.g., 30 -> "INFO")
                level_num = log_entry.get("level", 30)
                level = LEVEL_MAPPING.get(level_num, "INFO")
                message = log_entry.get("msg") or log_entry.get("message", "")

                if timestamp and message:
                    data.append([timestamp, level, message])
            except json.JSONDecodeError:
                continue # Skip lines that aren't valid JSON

    df = pd.DataFrame(data, columns=["timestamp", "level", "message"])
    if df.empty:
        print("‚ùå ERROR: No logs found! Check if the file contains valid JSON.")
    return df

def preprocess_data(df):
    # Your JSON logs use Unix timestamps (milliseconds), so we update the unit
    df["timestamp"] = pd.to_datetime(df["timestamp"], unit='ms', errors='coerce')
    df_clean = df.dropna(subset=["timestamp"]).copy()
    df_clean["level_score"] = df_clean["level"].replace(LEVEL_MAPPING).fillna(1)
    df_clean["message_length"] = df_clean["message"].apply(len)
    return df_clean

def detect_anomalies(df, CONTAMINATION_RATE):
    model = IsolationForest(contamination=CONTAMINATION_RATE, random_state=42)
    df["anomaly_code"] = model.fit_predict(df[["level_score", "message_length"]])
    df["status"] = df["anomaly_code"].apply(lambda x: "‚ùå ANOMALY" if x == -1 else "‚úÖ NORMAL")
    print(f"\n‚úÖ Analysis Complete! Processed {len(df)} log lines.")
    anomalies = df[df["status"] == "‚ùå ANOMALY"]
    if not anomalies.empty:
        print(anomalies[["timestamp", "level", "message", "status"]])
    else:
        print("No anomalies detected.")
    return df

def generate_report(df):
    colors = df['anomaly_code'].map({1: 'tab:green', -1: 'tab:red'})
    plt.figure(figsize=(10, 6))
    plt.scatter(df['level_score'], df['message_length'], c=colors, alpha=0.6)
    plt.title('AIOps Log Anomaly Detection')
    plt.xlabel('Log Severity (1=INFO, 4=CRITICAL)')
    plt.ylabel('Message Length (Characters)')
    plt.grid(True)
    plt.savefig('anomaly_report.png')
    print("\nüìà Graph saved successfully as 'anomaly_report.png'")

def trigger_alert(anomaly_count):
    if anomaly_count > 0:
        msg = f"AIOps ALERT: {anomaly_count} anomalies detected in system logs!"
        print(f"\nüîî SENDING NOTIFICATION: {msg}")

        os.system(f'notify-send "AIOps Alert" "{msg}"')

def send_slack_alert(anomaly_count, anomalies_df):
    if not SLACK_WEBHOOK_URL:
        print("‚ÑπÔ∏è SLACK_URL not set. Skipping Slack notification.")
        return

    if anomaly_count == 0:
        return

    summary = anomalies_df[['timestamp', 'level', 'message']].head(5).to_string(index=False)

    payload = {
        "text": "üö® *AIOps Anomaly Detection Alert* üö®",
        "attachments": [
            {
                "color": "#ff0000",
                "fields": [
                    {"title": "Total Anomalies Found", "value": str(anomaly_count), "short": True},
                    {"title": "Status", "value": "Action Required", "short": True},
                    {"title": "Top Anomalies Detected", "value": f"```{summary}```", "short": False}
                ],
                "footer": "Sent from AI-Log-Analyzer Engine"
            }
        ]
    }

    try:
        response = requests.post(SLACK_WEBHOOK_URL, data=json.dumps(payload))
        if response.status_code == 200:
            print("‚úÖ Slack notification sent!")
        else:
            print(f"‚ùå Failed to send Slack alert: {response.status_code}")
    except Exception as e:
        print(f"‚ö†Ô∏è Error connecting to Slack: {e}")


def main():
    df_raw_loads= load_and_parse_logs(LOG_FILE_PATH)
    if df_raw_loads.empty:
        return

    df_clean = preprocess_data(df_raw_loads)
    if df_clean.empty:
        print("‚ùå ERROR: No valid timestamps found. Check log format.")
        return

    df_final = detect_anomalies(df_clean, CONTAMINATION_RATE=percent_to_decimal(CONTAMINATION_RATE))
    anomalies = df_final[df_final["anomaly_code"] == -1]
    if not anomalies.empty:
        #trigger_alert(len(anomalies))
        send_slack_alert(len(anomalies), anomalies)

    #generate_report(df_final)

if __name__ == "__main__":
    main()
